{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e4cc7d10-d5ba-4a3a-bc15-2602a8a9e9ad",
    "_uuid": "769f3a6d-d073-488f-95f6-8c34c46fdcb5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-21T14:37:13.035578Z",
     "iopub.status.busy": "2025-04-21T14:37:13.034857Z",
     "iopub.status.idle": "2025-04-21T14:37:26.522220Z",
     "shell.execute_reply": "2025-04-21T14:37:26.521648Z",
     "shell.execute_reply.started": "2025-04-21T14:37:13.035552Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pydicom\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "81d8ce9c-160e-4af6-9209-17fa6874f3a2",
    "_uuid": "45ae8b8e-2e99-42f9-98d6-da96318acbaf",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-21T14:37:26.523850Z",
     "iopub.status.busy": "2025-04-21T14:37:26.523320Z",
     "iopub.status.idle": "2025-04-21T14:37:26.709951Z",
     "shell.execute_reply": "2025-04-21T14:37:26.709418Z",
     "shell.execute_reply.started": "2025-04-21T14:37:26.523822Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train  = pd.read_csv('/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train.csv')\n",
    "label = pd.read_csv('/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_label_coordinates.csv')\n",
    "train_desc = pd.read_csv('/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_series_descriptions.csv')\n",
    "test_desc = pd.read_csv('/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/test_series_descriptions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6e45dd1e-718d-4a87-8f90-7a08bfc23f0f",
    "_uuid": "d01ff453-8a71-4f8b-9818-44a398a9fd88",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-21T14:37:29.354754Z",
     "iopub.status.busy": "2025-04-21T14:37:29.354214Z",
     "iopub.status.idle": "2025-04-21T14:37:29.385220Z",
     "shell.execute_reply": "2025-04-21T14:37:29.384612Z",
     "shell.execute_reply.started": "2025-04-21T14:37:29.354732Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "40d691f4-e99e-48f7-a8a2-c6ece549a180",
    "_uuid": "b95b96b1-4802-46b1-9fd7-b8913599180d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-21T14:37:30.514467Z",
     "iopub.status.busy": "2025-04-21T14:37:30.513966Z",
     "iopub.status.idle": "2025-04-21T14:37:30.521447Z",
     "shell.execute_reply": "2025-04-21T14:37:30.520863Z",
     "shell.execute_reply.started": "2025-04-21T14:37:30.514441Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_desc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e903bd74-eb3d-4d47-81eb-9599e0147f23",
    "_uuid": "a452b332-b05a-48e8-bee5-c5fad1b4c95f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-21T14:40:28.536783Z",
     "iopub.status.busy": "2025-04-21T14:40:28.536545Z",
     "iopub.status.idle": "2025-04-21T14:40:28.934333Z",
     "shell.execute_reply": "2025-04-21T14:40:28.933604Z",
     "shell.execute_reply.started": "2025-04-21T14:40:28.536767Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def reshape_dataframe(df):\n",
    "    # Create a list of columns to exclude\n",
    "    exclude_columns = ['study_id', 'series_id', 'instance_number', 'x', 'y', 'series_description']\n",
    "    \n",
    "    # Filter the columns to process\n",
    "    columns_to_process = [col for col in df.columns if col not in exclude_columns]\n",
    "    \n",
    "    # Split the columns into condition and level, extract severity, and concatenate to form the new DataFrame\n",
    "    reshaped_df = pd.DataFrame([\n",
    "        {\n",
    "            'study_id': row['study_id'],\n",
    "            'condition': ' '.join([word.capitalize() for word in col.split('_')[:-2]]),\n",
    "            'level': col.split('_')[-2].capitalize() + '/' + col.split('_')[-1].capitalize(),\n",
    "            'severity': row[col]\n",
    "        }\n",
    "        for _, row in df.iterrows()\n",
    "        for col in columns_to_process\n",
    "    ])\n",
    "    \n",
    "    return reshaped_df\n",
    "\n",
    "# Reshape the DataFrame\n",
    "new_train_df = reshape_dataframe(train)\n",
    "\n",
    "# Display the first few rows of the reshaped DataFrame\n",
    "new_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "55c084b2-334f-4753-9fdd-c31fff84dd45",
    "_uuid": "cdd5e746-b060-47f8-afdf-fee183c657e2",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-21T14:40:30.675121Z",
     "iopub.status.busy": "2025-04-21T14:40:30.674475Z",
     "iopub.status.idle": "2025-04-21T14:40:30.679557Z",
     "shell.execute_reply": "2025-04-21T14:40:30.678853Z",
     "shell.execute_reply.started": "2025-04-21T14:40:30.675096Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Print columns in a neat way\n",
    "print(\"\\nColumns in new_train_df:\")\n",
    "print(\",\".join(new_train_df.columns))\n",
    "\n",
    "print(\"\\nColumns in label:\")\n",
    "print(\",\".join(label.columns))\n",
    "\n",
    "print(\"\\nColumns in test_desc:\")\n",
    "print(\",\".join(test_desc.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge reshaped labels with coordinate labels\n",
    "merged_df = pd.merge(\n",
    "    new_train_df,\n",
    "    label,\n",
    "    on=['study_id', 'condition', 'level'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Quick sanity check\n",
    "print(\"Merged rows:\", len(merged_df))\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "456408f9-ce58-4cc8-bc12-b5d5b33d5315",
    "_uuid": "a876f3a2-e86b-45f5-908f-49f7d58b8f02",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-21T14:40:36.593702Z",
     "iopub.status.busy": "2025-04-21T14:40:36.593423Z",
     "iopub.status.idle": "2025-04-21T14:40:36.615868Z",
     "shell.execute_reply": "2025-04-21T14:40:36.615184Z",
     "shell.execute_reply.started": "2025-04-21T14:40:36.593679Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Merge the dataframes on the common column 'series_id'\n",
    "final_merged_df = pd.merge(merged_df, train_desc, on=['series_id','study_id'], how='inner')\n",
    "# Display the first few rows of the final merged dataframe\n",
    "final_merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "390329b2-ad78-4dce-9961-5f1230f92935",
    "_uuid": "2ab51b9b-5811-4655-a7d3-982ce5eccfec",
    "trusted": true
   },
   "source": [
    "In medical imaging for spinal conditions, specific MRI sequences are often used to identify different types of spinal stenosis:\n",
    "\n",
    "- **Sagittal T1-weighted images** are primarily utilized to evaluate **Neural Foraminal Narrowing**. \n",
    "- **Axial T2-weighted images** are crucial for assessing **Subarticular Stenosis**. \n",
    "- **Sagittal T2-weighted or STIR (Short Tau Inversion Recovery) images** are typically employed to detect and analyze **Spinal Canal Stenosis**.\n",
    "\n",
    "These imaging sequences are chosen for their ability to provide the most relevant anatomical and pathological information for each specific type of stenosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "aafc3ed4-b4d1-4b1d-adb1-3bbb3d4a4306",
    "_uuid": "337a3e61-c05b-4511-aa53-3624ae3ffe93",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-21T14:40:52.404500Z",
     "iopub.status.busy": "2025-04-21T14:40:52.404024Z",
     "iopub.status.idle": "2025-04-21T14:40:52.558009Z",
     "shell.execute_reply": "2025-04-21T14:40:52.556932Z",
     "shell.execute_reply.started": "2025-04-21T14:40:52.404476Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create the row_id column\n",
    "final_merged_df['row_id'] = (\n",
    "    final_merged_df['study_id'].astype(str) + '_' +\n",
    "    final_merged_df['condition'].str.lower().str.replace(' ', '_') + '_' +\n",
    "    final_merged_df['level'].str.lower().str.replace('/', '_')\n",
    ")\n",
    "\n",
    "# Create the image_path column\n",
    "final_merged_df['image_path'] = (\n",
    "    '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images/' + \n",
    "    final_merged_df['study_id'].astype(str) + '/' +\n",
    "    final_merged_df['series_id'].astype(str) + '/' +\n",
    "    final_merged_df['instance_number'].astype(str) + '.dcm'\n",
    ")\n",
    "\n",
    "# Note: Check image path, since there's 1 instance id, for 1 image, but there's many more images other than the ones labelled in the instance ID. \n",
    "\n",
    "# Display the updated dataframe\n",
    "final_merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7897c7f7-85d6-46cc-85db-cec4c363eee4",
    "_uuid": "b03c8c55-40a7-4df1-a7c6-4c3c542eefc9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-21T14:40:55.295771Z",
     "iopub.status.busy": "2025-04-21T14:40:55.295503Z",
     "iopub.status.idle": "2025-04-21T14:40:55.706097Z",
     "shell.execute_reply": "2025-04-21T14:40:55.704995Z",
     "shell.execute_reply.started": "2025-04-21T14:40:55.295752Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the base path for test images\n",
    "base_path = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/test_images'\n",
    "\n",
    "# Function to get image paths for a series\n",
    "def get_image_paths(row):\n",
    "    series_path = os.path.join(base_path, str(row['study_id']), str(row['series_id']))\n",
    "    if os.path.exists(series_path):\n",
    "        return [os.path.join(series_path, f) for f in os.listdir(series_path) if os.path.isfile(os.path.join(series_path, f))]\n",
    "    return []\n",
    "\n",
    "# Mapping of series_description to conditions\n",
    "condition_mapping = {\n",
    "    'Sagittal T1': {'left': 'left_neural_foraminal_narrowing', 'right': 'right_neural_foraminal_narrowing'},\n",
    "    'Axial T2': {'left': 'left_subarticular_stenosis', 'right': 'right_subarticular_stenosis'},\n",
    "    'Sagittal T2/STIR': 'spinal_canal_stenosis'\n",
    "}\n",
    "\n",
    "# Create a list to store the expanded rows\n",
    "expanded_rows = []\n",
    "\n",
    "# Expand the dataframe by adding new rows for each file path\n",
    "for index, row in test_desc.iterrows():\n",
    "    image_paths = get_image_paths(row)\n",
    "    conditions = condition_mapping.get(row['series_description'], {})\n",
    "    if isinstance(conditions, str):  # Single condition\n",
    "        conditions = {'left': conditions, 'right': conditions}\n",
    "    for side, condition in conditions.items():\n",
    "        for image_path in image_paths:\n",
    "            expanded_rows.append({\n",
    "                'study_id': row['study_id'],\n",
    "                'series_id': row['series_id'],\n",
    "                'series_description': row['series_description'],\n",
    "                'image_path': image_path,\n",
    "                'condition': condition,\n",
    "                'row_id': f\"{row['study_id']}_{condition}\"\n",
    "            })\n",
    "\n",
    "# Create a new dataframe from the expanded rows\n",
    "expanded_test_desc = pd.DataFrame(expanded_rows)\n",
    "\n",
    "# Display the resulting dataframe\n",
    "expanded_test_desc.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d98e8037-d2d7-4025-834f-c4119ec2b08e",
    "_uuid": "596d6d58-0014-4adb-8147-3e3b93786fd6",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-21T14:40:59.354653Z",
     "iopub.status.busy": "2025-04-21T14:40:59.354363Z",
     "iopub.status.idle": "2025-04-21T14:40:59.362055Z",
     "shell.execute_reply": "2025-04-21T14:40:59.361350Z",
     "shell.execute_reply.started": "2025-04-21T14:40:59.354633Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "final_merged_df['severity'] = final_merged_df['severity'].fillna('Normal/Mild')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7e44f186-126c-4919-8f11-2761c8b06bf1",
    "_uuid": "c7af545f-d5e5-4eb2-994d-cb97848b8546",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-21T14:41:00.116474Z",
     "iopub.status.busy": "2025-04-21T14:41:00.116216Z",
     "iopub.status.idle": "2025-04-21T14:41:00.119989Z",
     "shell.execute_reply": "2025-04-21T14:41:00.119195Z",
     "shell.execute_reply.started": "2025-04-21T14:41:00.116455Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_data = expanded_test_desc\n",
    "train_data = final_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "23dd5387-eac5-4dc4-afe1-6a41da1b85c2",
    "_uuid": "05140088-d100-4f96-9b65-edb0eec7ae23",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-21T14:41:00.974234Z",
     "iopub.status.busy": "2025-04-21T14:41:00.973513Z",
     "iopub.status.idle": "2025-04-21T14:41:00.994962Z",
     "shell.execute_reply": "2025-04-21T14:41:00.994414Z",
     "shell.execute_reply.started": "2025-04-21T14:41:00.974204Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "53a3cb20-8556-402b-80f7-935023f7af39",
    "_uuid": "cb3aadcc-9db1-4008-a386-7ed28cf483d0",
    "trusted": true
   },
   "source": [
    "Checking whether there are any errors or outliers in the co-ordinates that are necessary to remove or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "68ddfae7-7959-42c3-a1a6-edece55fdceb",
    "_uuid": "d98a81e1-8d6f-4d7f-95a3-3b280d0cf1a7",
    "trusted": true
   },
   "source": [
    "# Data Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "74229ce7-9bea-48f6-8ee8-bd2e37371534",
    "_uuid": "b9428fa6-02bf-45e2-97c0-365516a8faa6",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-21T14:41:06.894502Z",
     "iopub.status.busy": "2025-04-21T14:41:06.893842Z",
     "iopub.status.idle": "2025-04-21T14:41:06.951520Z",
     "shell.execute_reply": "2025-04-21T14:41:06.950821Z",
     "shell.execute_reply.started": "2025-04-21T14:41:06.894480Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Display basic statistics for 'x' and 'y' columns\n",
    "x_stats = train_data['x'].describe()\n",
    "y_stats = train_data['y'].describe()\n",
    "\n",
    "print(\"X Coordinate Statistics:\")\n",
    "print(x_stats)\n",
    "\n",
    "print(\"\\nY Coordinate Statistics:\")\n",
    "print(y_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d98c3da8-50e1-49f4-ae1e-8e710301c486",
    "_uuid": "0e6adc83-237d-42ee-aeb6-7b5e6763762a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-21T14:41:09.533745Z",
     "iopub.status.busy": "2025-04-21T14:41:09.533478Z",
     "iopub.status.idle": "2025-04-21T14:41:10.346176Z",
     "shell.execute_reply": "2025-04-21T14:41:10.344391Z",
     "shell.execute_reply.started": "2025-04-21T14:41:09.533725Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create a histogram for 'x' values\n",
    "x_hist = go.Histogram(\n",
    "    x=train_data['x'],\n",
    "    nbinsx=30,\n",
    "    name='X Coordinates',\n",
    "    marker_color='blue',\n",
    "    opacity=0.7\n",
    ")\n",
    "\n",
    "# Create a histogram for 'y' values\n",
    "y_hist = go.Histogram(\n",
    "    x=train_data['y'],\n",
    "    nbinsx=30,\n",
    "    name='Y Coordinates',\n",
    "    marker_color='green',\n",
    "    opacity=0.7\n",
    ")\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('Distribution of X Coordinates', 'Distribution of Y Coordinates'))\n",
    "\n",
    "# Add the histograms to the figure\n",
    "fig.add_trace(x_hist, row=1, col=1)\n",
    "fig.add_trace(y_hist, row=1, col=2)\n",
    "\n",
    "# Update layout for a cleaner look\n",
    "fig.update_layout(\n",
    "    title_text=\"Distribution of X and Y Coordinates\",\n",
    "    showlegend=False,\n",
    "    xaxis_title=\"X Values\",\n",
    "    yaxis_title=\"Frequency\",\n",
    "    xaxis2_title=\"Y Values\",\n",
    "    yaxis2_title=\"Frequency\",\n",
    "    bargap=0.2,  # Gap between bars\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "547bdb5d-032c-45e3-b9cc-9ff07770f012",
    "_uuid": "76a2f293-ffe9-4344-bf76-659ded8f4e74",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-21T14:41:25.013992Z",
     "iopub.status.busy": "2025-04-21T14:41:25.013277Z",
     "iopub.status.idle": "2025-04-21T14:41:25.076807Z",
     "shell.execute_reply": "2025-04-21T14:41:25.076189Z",
     "shell.execute_reply.started": "2025-04-21T14:41:25.013966Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Count the occurrences of each severity within each condition\n",
    "severity_condition_counts = train_data.groupby(['condition', 'severity']).size().reset_index(name='count')\n",
    "\n",
    "# Create a grouped bar chart\n",
    "fig = px.bar(\n",
    "    severity_condition_counts,\n",
    "    x='condition',\n",
    "    y='count',\n",
    "    color='severity',\n",
    "    barmode='group',\n",
    "    title='Distribution of Severities for Each Condition',\n",
    "    labels={'condition': 'Condition', 'count': 'Number of Cases', 'severity': 'Severity'},\n",
    "    color_discrete_sequence=px.colors.qualitative.Set1  # Custom color sequence\n",
    ")\n",
    "\n",
    "# Update the layout for better presentation\n",
    "fig.update_layout(\n",
    "    xaxis_title='Condition',\n",
    "    yaxis_title='Number of Cases',\n",
    "    legend_title='Severity',\n",
    "    bargap=0.15,\n",
    "    bargroupgap=0.1\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2d053601-ff9f-4855-9d48-08c894603e99",
    "_uuid": "7c5b6cf6-b860-4cc0-a882-00b144b40e96",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-21T14:41:27.614780Z",
     "iopub.status.busy": "2025-04-21T14:41:27.614511Z",
     "iopub.status.idle": "2025-04-21T14:41:27.682424Z",
     "shell.execute_reply": "2025-04-21T14:41:27.681696Z",
     "shell.execute_reply.started": "2025-04-21T14:41:27.614761Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Count the occurrences of each severity within each condition\n",
    "severity_condition_counts = train_data.groupby(['condition', 'series_description']).size().reset_index(name='count')\n",
    "\n",
    "# Create a grouped bar chart\n",
    "fig = px.bar(\n",
    "    severity_condition_counts,\n",
    "    x='condition',\n",
    "    y='count',\n",
    "    color='series_description',\n",
    "    barmode='group',\n",
    "    title='Distribution of Condition for Respective Angle',\n",
    "    labels={'condition': 'Condition', 'count': 'Number of Cases', 'series_description': 'Angle of MR Image'},\n",
    "    color_discrete_sequence=px.colors.qualitative.Set1  # Custom color sequence\n",
    ")\n",
    "\n",
    "# Update the layout for better presentation\n",
    "fig.update_layout(\n",
    "    xaxis_title='Condition',\n",
    "    yaxis_title='Number of Cases',\n",
    "    legend_title='Angle',\n",
    "    bargap=0.15,\n",
    "    bargroupgap=0.1\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5a0331a5-d395-4fea-8f6f-24e2b872cad7",
    "_uuid": "dc2582c7-942e-49e5-a10e-f3f0d044727e",
    "trusted": true
   },
   "source": [
    "In medical imaging for spinal conditions, specific MRI sequences are often used to identify different types of spinal stenosis:\n",
    "\n",
    "- **Sagittal T1-weighted images** are primarily utilized to evaluate **Neural Foraminal Narrowing**. \n",
    "- **Axial T2-weighted images** are crucial for assessing **Subarticular Stenosis**. \n",
    "- **Sagittal T2-weighted or STIR (Short Tau Inversion Recovery) images** are typically employed to detect and analyze **Spinal Canal Stenosis**.\n",
    "\n",
    "These imaging sequences are chosen for their ability to provide the most relevant anatomical and pathological information for each specific type of stenosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fa01ff2a-2ad3-48db-ab44-5b6e0b98326a",
    "_uuid": "694c40e4-9768-4dd9-b3fc-8670f0d5edb1",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-21T14:41:32.618949Z",
     "iopub.status.busy": "2025-04-21T14:41:32.618487Z",
     "iopub.status.idle": "2025-04-21T14:41:32.678079Z",
     "shell.execute_reply": "2025-04-21T14:41:32.677374Z",
     "shell.execute_reply.started": "2025-04-21T14:41:32.618927Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Group by 'level' and 'condition' and count the occurrences\n",
    "level_condition_counts = train_data.groupby(['condition', 'level']).size().reset_index(name='count')\n",
    "\n",
    "# Create a grouped bar chart\n",
    "fig = px.bar(\n",
    "    level_condition_counts,\n",
    "    x='condition',\n",
    "    y='count',\n",
    "    color='level',\n",
    "    barmode='group',\n",
    "    title='Distribution of Levels for Each Condition',\n",
    "    labels={'condition': 'Condition', 'count': 'Number of Cases', 'level': 'Level'},\n",
    "    color_discrete_sequence=px.colors.qualitative.Set1  # Custom color sequence\n",
    ")\n",
    "\n",
    "# Update the layout for better presentation\n",
    "fig.update_layout(\n",
    "    xaxis_title='Condition',\n",
    "    yaxis_title='Number of Cases',\n",
    "    legend_title='Level',\n",
    "    bargap=0.15,\n",
    "    bargroupgap=0.1\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3ed6c023-4da8-47cb-a8ad-534a61157724",
    "_uuid": "1ad5c50f-ca9c-4f64-ba8b-4887bf63efb0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-21T14:41:36.176555Z",
     "iopub.status.busy": "2025-04-21T14:41:36.175989Z",
     "iopub.status.idle": "2025-04-21T14:41:36.257433Z",
     "shell.execute_reply": "2025-04-21T14:41:36.256776Z",
     "shell.execute_reply.started": "2025-04-21T14:41:36.176531Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Count the occurrences of each level within each condition\n",
    "level_condition_counts = train_data.groupby(['level', 'condition']).size().reset_index(name='count')\n",
    "\n",
    "# Create a pivot table to structure the data for the heatmap\n",
    "heatmap_data = level_condition_counts.pivot(index='level', columns='condition', values='count')\n",
    "\n",
    "# Create the heatmap\n",
    "fig = px.imshow(\n",
    "    heatmap_data,\n",
    "    labels={'x': 'Condition', 'y': 'Level', 'color': 'Count'},\n",
    "    title='Heatmap of Levels by Condition',\n",
    "    color_continuous_scale='Viridis'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "003c6c07-be21-418b-96a0-7d4c8016526d",
    "_uuid": "a62a3e46-b1bb-463e-a524-35320211c967",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-19T18:13:29.012108Z",
     "iopub.status.busy": "2025-04-19T18:13:29.011432Z",
     "iopub.status.idle": "2025-04-19T18:13:29.546513Z",
     "shell.execute_reply": "2025-04-19T18:13:29.545738Z",
     "shell.execute_reply.started": "2025-04-19T18:13:29.012086Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Export the DataFrame to a CSV file\n",
    "final_merged_df.to_csv('train_processed.csv', index=False)\n",
    "test_data.to_csv('test_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and SMOTE Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:13:32.337121Z",
     "iopub.status.busy": "2025-04-19T18:13:32.336414Z",
     "iopub.status.idle": "2025-04-19T18:13:33.197147Z",
     "shell.execute_reply": "2025-04-19T18:13:33.196480Z",
     "shell.execute_reply.started": "2025-04-19T18:13:32.337095Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Mapping for the 'level' column\n",
    "level_mapping = {'L1/L2': 0, 'L2/L3': 1, 'L3/L4': 2, 'L4/L5': 3, 'L5/S1': 4}\n",
    "final_merged_df['level_encoded'] = final_merged_df['level'].map(level_mapping)\n",
    "\n",
    "# Mapping for the 'series_description' column\n",
    "series_description_mapping = {'Sagittal T1': 0, 'Sagittal T2/STIR': 1, 'Axial T2': 2}\n",
    "final_merged_df['series_description_encoded'] = final_merged_df['series_description'].map(series_description_mapping)\n",
    "\n",
    "# Convert categorical features to numerical using LabelEncoder for 'condition' and 'severity'\n",
    "le_condition = LabelEncoder()\n",
    "le_severity = LabelEncoder()\n",
    "\n",
    "final_merged_df['condition_encoded'] = le_condition.fit_transform(final_merged_df['condition'])\n",
    "final_merged_df['severity_encoded'] = le_severity.fit_transform(final_merged_df['severity'])\n",
    "\n",
    "# Concatenate condition and severity labels\n",
    "final_merged_df['combined_label'] = final_merged_df['condition_encoded'].astype(str) + \"_\" + final_merged_df['severity_encoded'].astype(str)\n",
    "\n",
    "# Remove non-numeric columns and use only relevant features\n",
    "X = final_merged_df.drop(['condition', 'severity', 'row_id', 'image_path', 'level', 'series_description', 'condition_encoded', 'severity_encoded', 'combined_label'], axis=1)\n",
    "X['level'] = final_merged_df['level_encoded']  # Add the newly encoded 'level' column\n",
    "X['series_description'] = final_merged_df['series_description_encoded']  # Add the newly encoded 'series_description' column\n",
    "\n",
    "# SMOTE: Apply SMOTE to the concatenated labels (condition + severity)\n",
    "y_combined = final_merged_df['combined_label']\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_combined_smote = smote.fit_resample(X, y_combined)\n",
    "\n",
    "# Split back the combined label into separate condition and severity labels\n",
    "y_condition_smote = y_combined_smote.str.split(\"_\", expand=True)[0].astype(int)  # Condition part\n",
    "y_severity_smote = y_combined_smote.str.split(\"_\", expand=True)[1].astype(int)  # Severity part\n",
    "\n",
    "# One-hot encode the target labels for multi-class classification\n",
    "onehot = OneHotEncoder()\n",
    "\n",
    "y_condition_smote_oh = onehot.fit_transform(y_condition_smote.values.reshape(-1, 1)).toarray()\n",
    "y_severity_smote_oh = onehot.fit_transform(y_severity_smote.values.reshape(-1, 1)).toarray()\n",
    "\n",
    "# Print the shapes to verify correctness\n",
    "print(f\"Shape of X_smote: {X_smote.shape}\")\n",
    "print(f\"Shape of y_condition_smote_oh: {y_condition_smote_oh.shape}\")\n",
    "print(f\"Shape of y_severity_smote_oh: {y_severity_smote_oh.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, categorical features like level and series_description are mapped to numeric values. The target variables condition and severity are label-encoded and combined. SMOTE (Synthetic Minority Oversampling Technique) is applied to balance the dataset by generating synthetic samples for minority classes. After resampling, the combined labels are split back into separate condition and severity labels, which are then one-hot encoded for multi-class classification. The resulting dataset is now balanced and ready for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:13:38.838286Z",
     "iopub.status.busy": "2025-04-19T18:13:38.838009Z",
     "iopub.status.idle": "2025-04-19T18:13:38.876245Z",
     "shell.execute_reply": "2025-04-19T18:13:38.875530Z",
     "shell.execute_reply.started": "2025-04-19T18:13:38.838267Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Retain 'study_id', 'series_id', 'instance_number', 'x', 'y', and 'level' from the original dataset\n",
    "X_combined = final_merged_df[['study_id', 'series_id', 'instance_number', 'x', 'y', 'level']].copy()\n",
    "\n",
    "# Mapping for the 'level' column as specified\n",
    "level_mapping = {'L1/L2': 1, 'L2/L3': 2, 'L3/L4': 3, 'L4/L5': 4, 'L5/S1': 5}\n",
    "X_combined['level'] = X_combined['level'].map(level_mapping)\n",
    "\n",
    "# Step 2: Initialize y_condition_smote_df and apply condition mapping as specified\n",
    "condition_mapping = {\n",
    "    'Spinal Canal Stenosis': 1,\n",
    "    'Left Neural Foraminal Narrowing': 2,\n",
    "    'Right Neural Foraminal Narrowing': 3,\n",
    "    'Left Subarticular Stenosis': 4,\n",
    "    'Right Subarticular Stenosis': 5\n",
    "}\n",
    "\n",
    "# Assuming y_condition_smote_oh is available from SMOTE\n",
    "y_condition_smote_df = pd.DataFrame(y_condition_smote_oh, columns=[f\"condition_{i}\" for i in range(y_condition_smote_oh.shape[1])])\n",
    "\n",
    "# Apply the condition mapping\n",
    "y_condition_smote_df = y_condition_smote_df.replace(condition_mapping)\n",
    "\n",
    "# Step 3: Initialize y_severity_smote_df from SMOTE results\n",
    "# Assuming y_severity_smote_oh is available from SMOTE\n",
    "y_severity_smote_df = pd.DataFrame(y_severity_smote_oh, columns=[f\"severity_{i}\" for i in range(y_severity_smote_oh.shape[1])])\n",
    "\n",
    "# Step 4: Convert the integer-related columns in X_combined to actual integers\n",
    "X_combined['study_id'] = X_combined['study_id'].astype(int)\n",
    "X_combined['series_id'] = X_combined['series_id'].astype(int)\n",
    "X_combined['instance_number'] = X_combined['instance_number'].astype(int)\n",
    "X_combined['level'] = X_combined['level'].astype(int)\n",
    "\n",
    "# Keep 'x' and 'y' as float\n",
    "X_combined['x'] = X_combined['x'].astype(float)\n",
    "X_combined['y'] = X_combined['y'].astype(float)\n",
    "\n",
    "# Step 5: Initialize X_smote_df from SMOTE result\n",
    "X_smote_df = pd.DataFrame(X_smote, columns=[f\"feature_{i}\" for i in range(X_smote.shape[1])])\n",
    "\n",
    "# Step 6: Drop columns 'feature_0' to 'feature_8'\n",
    "X_smote_df.drop(columns=[f\"feature_{i}\" for i in range(9)], inplace=True)\n",
    "\n",
    "# Step 7: Combine SMOTE features with 'study_id', 'series_id', 'instance_number', 'x', 'y', and 'level'\n",
    "X_smote_combined_df = pd.concat([X_combined, X_smote_df], axis=1)\n",
    "\n",
    "# Step 8: Drop rows where all values are NaN/null\n",
    "X_smote_combined_df.dropna(how='all', inplace=True)\n",
    "\n",
    "# Step 9: Prevent scientific notation for integer columns and save as CSV\n",
    "pd.set_option('display.float_format', '{:.0f}'.format)  # Disable scientific notation for integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`condition_0`: **Spinal Canal Stenosis**\n",
    "\n",
    "`condition_1`: **Left Neural Foraminal Narrowing**\n",
    "\n",
    "`condition_2`: **Right Neural Foraminal Narrowing**\n",
    "\n",
    "`condition_3`: **Left Subarticular Stenosis**\n",
    "\n",
    "`condition_4`: **Right Subarticular Stenosis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`severity_0`: **Normal/Mild**\n",
    "\n",
    "`severity_1`: **Moderate**\n",
    "\n",
    "`severity_2`: **Severe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:14:01.422094Z",
     "iopub.status.busy": "2025-04-19T18:14:01.421520Z",
     "iopub.status.idle": "2025-04-19T18:14:01.426925Z",
     "shell.execute_reply": "2025-04-19T18:14:01.426020Z",
     "shell.execute_reply.started": "2025-04-19T18:14:01.422069Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Check the number of samples\n",
    "print(f\"Filtered X_smote_combined_df size: {len(X_smote_combined_df)}\")  # Should be 48,692\n",
    "\n",
    "# Ensure the labels are of the same size as X_smote_combined_df by filtering the corresponding labels\n",
    "# Make sure this aligns with how rows were dropped earlier in X_smote_combined_df\n",
    "y_condition_smote_df_filtered = y_condition_smote_df.iloc[:len(X_smote_combined_df)]\n",
    "y_severity_smote_df_filtered = y_severity_smote_df.iloc[:len(X_smote_combined_df)]\n",
    "\n",
    "# Now check the sizes\n",
    "print(f\"y_condition_smote_df size after filtering: {len(y_condition_smote_df_filtered)}\")  # Should match 48,692\n",
    "print(f\"y_severity_smote_df size after filtering: {len(y_severity_smote_df_filtered)}\")    # Should match 48,692"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-19T18:14:05.537546Z",
     "iopub.status.busy": "2025-04-19T18:14:05.537027Z",
     "iopub.status.idle": "2025-04-19T18:14:06.300766Z",
     "shell.execute_reply": "2025-04-19T18:14:06.300130Z",
     "shell.execute_reply.started": "2025-04-19T18:14:05.537527Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the combined DataFrame and labels to CSV files\n",
    "X_smote_combined_df.to_csv('/kaggle/working/X_smote_combined.csv', index=False)\n",
    "y_condition_smote_df.to_csv('/kaggle/working/y_condition_smote_oh.csv', index=False)\n",
    "y_severity_smote_df.to_csv('/kaggle/working/y_severity_smote_oh.csv', index=False)\n",
    "\n",
    "# Confirm the export\n",
    "print(\"/kaggle/working/X_smote_combined.csv, /kaggle/working/y_condition_smote_oh.csv, /kaggle/working/y_severity_smote_oh.csv exported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing & Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:14:11.122045Z",
     "iopub.status.busy": "2025-04-19T18:14:11.121495Z",
     "iopub.status.idle": "2025-04-19T18:14:11.133536Z",
     "shell.execute_reply": "2025-04-19T18:14:11.132910Z",
     "shell.execute_reply.started": "2025-04-19T18:14:11.122020Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pydicom\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "# Data generator class\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, df, image_folder, y_condition, y_severity, batch_size=32, img_size=(224, 224)):\n",
    "        self.df = df\n",
    "        self.image_folder = image_folder\n",
    "        self.y_condition = y_condition\n",
    "        self.y_severity = y_severity\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.indices = np.arange(len(df))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Generate batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        \n",
    "        # Get the batch data\n",
    "        batch_df = self.df.iloc[batch_indices]\n",
    "        batch_y_condition = self.y_condition[batch_indices]\n",
    "        batch_y_severity = self.y_severity[batch_indices]\n",
    "        \n",
    "        # Load the images for the batch\n",
    "        images = []\n",
    "        for _, row in batch_df.iterrows():\n",
    "            study_id = int(row['study_id'])  # Ensure integer format\n",
    "            series_id = int(row['series_id'])  # Ensure integer format\n",
    "            instance_number = int(row['instance_number'])  # Ensure integer format\n",
    "            \n",
    "            # Construct image path using integer values (convert to string)\n",
    "            img_path = os.path.join(self.image_folder, str(study_id), str(series_id), f\"{instance_number}.dcm\")\n",
    "            \n",
    "            # Load and preprocess the DICOM image\n",
    "            img = self.load_dicom_image(img_path)\n",
    "            images.append(img)\n",
    "        \n",
    "        return np.array(images), {'condition_output': batch_y_condition, 'severity_output': batch_y_severity}\n",
    "    \n",
    "    def load_dicom_image(self, image_path):\n",
    "        try:\n",
    "            dicom = pydicom.dcmread(image_path)\n",
    "            if hasattr(dicom, 'pixel_array'):\n",
    "                img = dicom.pixel_array\n",
    "                img = cv2.resize(img, self.img_size)  # Resize to 224x224\n",
    "                img = np.stack((img,) * 3, axis=-1)  # Convert to RGB (3 channels)\n",
    "                img = img / 255.0  # Normalize\n",
    "                return img\n",
    "            else:\n",
    "                print(f\"Warning: No pixel data in DICOM file: {image_path}\")\n",
    "                return np.zeros((*self.img_size, 3))  # Return a blank image if no pixel data\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading DICOM file: {image_path}. Error: {e}\")\n",
    "            return np.zeros((*self.img_size, 3))  # Return a blank image if any error occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the EfficientNetB0 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:14:23.827751Z",
     "iopub.status.busy": "2025-04-19T18:14:23.827222Z",
     "iopub.status.idle": "2025-04-19T18:14:23.833150Z",
     "shell.execute_reply": "2025-04-19T18:14:23.832462Z",
     "shell.execute_reply.started": "2025-04-19T18:14:23.827728Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Function to build the EfficientNetB0 model\n",
    "def build_efficientnet_model(num_classes_condition, num_classes_severity, input_shape=(224, 224, 3)):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    base_model = EfficientNetB0(include_top=False, weights='imagenet', input_tensor=inputs)\n",
    "    \n",
    "    # Add global pooling and dense layers for both condition and severity\n",
    "    x = Flatten()(base_model.output)\n",
    "    \n",
    "    # Output for condition classification\n",
    "    condition_output = Dense(num_classes_condition, activation='softmax', name='condition_output')(x)\n",
    "    \n",
    "    # Output for severity classification\n",
    "    severity_output = Dense(num_classes_severity, activation='softmax', name='severity_output')(x)\n",
    "    \n",
    "    # Define the model\n",
    "    model = Model(inputs, outputs=[condition_output, severity_output])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss={'condition_output': 'categorical_crossentropy', 'severity_output': 'categorical_crossentropy'},\n",
    "                  metrics={'condition_output': 'accuracy', 'severity_output': 'accuracy'})\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build RNN Model with LSTM for Sequential Medical Image Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dropout, TimeDistributed, Reshape\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Build RNN/LSTM model for sequential analysis of lumbar spine images\n",
    "def build_rnn_model(num_classes_condition, num_classes_severity, input_shape=(224, 224, 3), timesteps=10):\n",
    "    \"\"\"\n",
    "    RNN model with LSTM layers for analyzing sequential DICOM images\n",
    "    timesteps: number of sequential images to analyze (e.g., slices from MRI scan)\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Reshape input to add time dimension\n",
    "        TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape=(timesteps, *input_shape)),\n",
    "        TimeDistributed(MaxPooling2D((2, 2))),\n",
    "        TimeDistributed(Conv2D(64, (3, 3), activation='relu')),\n",
    "        TimeDistributed(MaxPooling2D((2, 2))),\n",
    "        TimeDistributed(Conv2D(128, (3, 3), activation='relu')),\n",
    "        TimeDistributed(MaxPooling2D((2, 2))),\n",
    "        TimeDistributed(Flatten()),\n",
    "        \n",
    "        # Bidirectional LSTM layers to capture temporal patterns\n",
    "        Bidirectional(LSTM(256, return_sequences=True)),\n",
    "        Dropout(0.3),\n",
    "        Bidirectional(LSTM(128, return_sequences=False)),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Dense layers for classification\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu')\n",
    "    ])\n",
    "    \n",
    "    # Add output layers for condition and severity\n",
    "    inputs = model.input\n",
    "    x = model.output\n",
    "    \n",
    "    condition_output = Dense(num_classes_condition, activation='softmax', name='condition_output')(x)\n",
    "    severity_output = Dense(num_classes_severity, activation='softmax', name='severity_output')(x)\n",
    "    \n",
    "    # Create the final model\n",
    "    rnn_model = Model(inputs=inputs, outputs=[condition_output, severity_output])\n",
    "    \n",
    "    # Compile the model\n",
    "    rnn_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss={'condition_output': 'categorical_crossentropy', 'severity_output': 'categorical_crossentropy'},\n",
    "        metrics={'condition_output': 'accuracy', 'severity_output': 'accuracy'}\n",
    "    )\n",
    "    \n",
    "    return rnn_model\n",
    "\n",
    "# Print model architecture\n",
    "print(\"RNN/LSTM Model Architecture:\")\n",
    "print(\"This model analyzes sequential medical images to capture temporal patterns in lumbar spine conditions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Simpler RNN model using GRU (faster training)\n",
    "from tensorflow.keras.layers import GRU, BatchNormalization\n",
    "\n",
    "def build_gru_model(num_classes_condition, num_classes_severity, sequence_length=10, feature_dim=128):\n",
    "    \"\"\"\n",
    "    Simplified GRU-based model for sequential feature analysis\n",
    "    Works with pre-extracted features from images\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=(sequence_length, feature_dim))\n",
    "    \n",
    "    # GRU layers\n",
    "    x = Bidirectional(GRU(128, return_sequences=True))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Bidirectional(GRU(64, return_sequences=False))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Dense layers\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Output layers\n",
    "    condition_output = Dense(num_classes_condition, activation='softmax', name='condition_output')(x)\n",
    "    severity_output = Dense(num_classes_severity, activation='softmax', name='severity_output')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=[condition_output, severity_output])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss={'condition_output': 'categorical_crossentropy', 'severity_output': 'categorical_crossentropy'},\n",
    "        metrics={'condition_output': 'accuracy', 'severity_output': 'accuracy'}\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"GRU Model created for faster sequential processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Data Generator for Sequential Images\n",
    "\n",
    "Since lumbar spine MRI scans contain multiple slices (sequential images), we can leverage RNN/LSTM to analyze these sequences. This generator loads multiple consecutive DICOM slices as a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialDataGenerator(Sequence):\n",
    "    \"\"\"\n",
    "    Data generator for loading sequential DICOM images for RNN/LSTM models\n",
    "    Loads multiple consecutive slices from the same series\n",
    "    \"\"\"\n",
    "    def __init__(self, df, image_folder, y_condition, y_severity, batch_size=16, img_size=(224, 224), sequence_length=10):\n",
    "        self.df = df\n",
    "        self.image_folder = image_folder\n",
    "        self.y_condition = y_condition\n",
    "        self.y_severity = y_severity\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # Group by study_id and series_id to get sequences\n",
    "        self.grouped = df.groupby(['study_id', 'series_id'])\n",
    "        self.group_keys = list(self.grouped.groups.keys())\n",
    "        self.indices = np.arange(len(self.group_keys))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.group_keys) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_keys = [self.group_keys[i] for i in batch_indices]\n",
    "        \n",
    "        sequences = []\n",
    "        batch_y_condition = []\n",
    "        batch_y_severity = []\n",
    "        \n",
    "        for study_id, series_id in batch_keys:\n",
    "            group_df = self.grouped.get_group((study_id, series_id))\n",
    "            \n",
    "            # Sort by instance number to maintain sequence\n",
    "            group_df = group_df.sort_values('instance_number')\n",
    "            \n",
    "            # Load sequence of images\n",
    "            sequence = []\n",
    "            for _, row in group_df.head(self.sequence_length).iterrows():\n",
    "                img_path = os.path.join(\n",
    "                    self.image_folder, \n",
    "                    str(int(row['study_id'])), \n",
    "                    str(int(row['series_id'])), \n",
    "                    f\"{int(row['instance_number'])}.dcm\"\n",
    "                )\n",
    "                img = self.load_dicom_image(img_path)\n",
    "                sequence.append(img)\n",
    "            \n",
    "            # Pad sequence if shorter than sequence_length\n",
    "            while len(sequence) < self.sequence_length:\n",
    "                sequence.append(np.zeros((*self.img_size, 3)))\n",
    "            \n",
    "            sequences.append(sequence)\n",
    "            \n",
    "            # Get labels (use first row of the group)\n",
    "            first_idx = group_df.index[0]\n",
    "            batch_y_condition.append(self.y_condition[first_idx])\n",
    "            batch_y_severity.append(self.y_severity[first_idx])\n",
    "        \n",
    "        return (\n",
    "            np.array(sequences), \n",
    "            {\n",
    "                'condition_output': np.array(batch_y_condition), \n",
    "                'severity_output': np.array(batch_y_severity)\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def load_dicom_image(self, image_path):\n",
    "        try:\n",
    "            dicom = pydicom.dcmread(image_path)\n",
    "            if hasattr(dicom, 'pixel_array'):\n",
    "                img = dicom.pixel_array\n",
    "                img = cv2.resize(img, self.img_size)\n",
    "                img = np.stack((img,) * 3, axis=-1)\n",
    "                img = img / 255.0\n",
    "                return img\n",
    "            else:\n",
    "                return np.zeros((*self.img_size, 3))\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading DICOM file: {image_path}. Error: {e}\")\n",
    "            return np.zeros((*self.img_size, 3))\n",
    "\n",
    "print(\"Sequential Data Generator created for RNN models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train RNN/LSTM Model\n",
    "\n",
    "Training the RNN model with sequential DICOM images to capture temporal patterns across multiple slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image folder for training/validation generators\n",
    "# Update this path if your data lives elsewhere\n",
    "image_folder = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images'\n",
    "print('Using image folder:', image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for RNN training\n",
    "# Split data maintaining sequential structure\n",
    "X_train_rnn, X_val_rnn, y_train_cond_rnn, y_val_cond_rnn, y_train_sev_rnn, y_val_sev_rnn = train_test_split(\n",
    "    X_smote_combined_df, \n",
    "    y_condition_smote_df_filtered.values, \n",
    "    y_severity_smote_df_filtered.values, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"RNN Training set: {len(X_train_rnn)} samples\")\n",
    "print(f\"RNN Validation set: {len(X_val_rnn)} samples\")\n",
    "\n",
    "# Create sequential data generators\n",
    "train_seq_generator = SequentialDataGenerator(\n",
    "    X_train_rnn, \n",
    "    image_folder, \n",
    "    y_train_cond_rnn, \n",
    "    y_train_sev_rnn, \n",
    "    batch_size=16,\n",
    "    sequence_length=10\n",
    ")\n",
    "\n",
    "val_seq_generator = SequentialDataGenerator(\n",
    "    X_val_rnn, \n",
    "    image_folder, \n",
    "    y_val_cond_rnn, \n",
    "    y_val_sev_rnn, \n",
    "    batch_size=16,\n",
    "    sequence_length=10\n",
    ")\n",
    "\n",
    "# Build RNN model\n",
    "rnn_model = build_rnn_model(num_classes_condition=5, num_classes_severity=3, timesteps=10)\n",
    "\n",
    "# Display model summary\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RNN model\n",
    "rnn_earlystop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "rnn_checkpoint = ModelCheckpoint('rnn_lstm_model.keras', save_best_only=True, verbose=1)\n",
    "\n",
    "# Reduce learning rate on plateau\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1)\n",
    "\n",
    "print(\"Starting RNN/LSTM model training...\")\n",
    "print(\"This model analyzes sequential MRI slices to capture temporal patterns.\")\n",
    "\n",
    "rnn_history = rnn_model.fit(\n",
    "    train_seq_generator,\n",
    "    validation_data=val_seq_generator,\n",
    "    epochs=15,\n",
    "    callbacks=[rnn_earlystop, rnn_checkpoint, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nRNN Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RNN training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Condition accuracy\n",
    "axes[0, 0].plot(rnn_history.history['condition_output_accuracy'], label='Train')\n",
    "axes[0, 0].plot(rnn_history.history['val_condition_output_accuracy'], label='Validation')\n",
    "axes[0, 0].set_title('RNN Condition Classification Accuracy')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Condition loss\n",
    "axes[0, 1].plot(rnn_history.history['condition_output_loss'], label='Train')\n",
    "axes[0, 1].plot(rnn_history.history['val_condition_output_loss'], label='Validation')\n",
    "axes[0, 1].set_title('RNN Condition Classification Loss')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Severity accuracy\n",
    "axes[1, 0].plot(rnn_history.history['severity_output_accuracy'], label='Train')\n",
    "axes[1, 0].plot(rnn_history.history['val_severity_output_accuracy'], label='Validation')\n",
    "axes[1, 0].set_title('RNN Severity Classification Accuracy')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Severity loss\n",
    "axes[1, 1].plot(rnn_history.history['severity_output_loss'], label='Train')\n",
    "axes[1, 1].plot(rnn_history.history['val_severity_output_loss'], label='Validation')\n",
    "axes[1, 1].set_title('RNN Severity Classification Loss')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"RNN training visualization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate RNN model on validation data\n",
    "print(\"Evaluating RNN model on validation set...\")\n",
    "rnn_results = rnn_model.evaluate(val_seq_generator, verbose=1)\n",
    "\n",
    "print(\"\\n=== RNN Model Performance ===\")\n",
    "print(f\"Total Loss: {rnn_results[0]:.4f}\")\n",
    "print(f\"Condition Loss: {rnn_results[1]:.4f}\")\n",
    "print(f\"Severity Loss: {rnn_results[2]:.4f}\")\n",
    "print(f\"Condition Accuracy: {rnn_results[3]:.4f}\")\n",
    "print(f\"Severity Accuracy: {rnn_results[4]:.4f}\")\n",
    "\n",
    "# Get predictions for detailed analysis\n",
    "print(\"\\nGenerating predictions...\")\n",
    "rnn_pred_condition, rnn_pred_severity = rnn_model.predict(val_seq_generator)\n",
    "\n",
    "# Convert to class labels\n",
    "rnn_pred_condition_classes = rnn_pred_condition.argmax(axis=1)\n",
    "rnn_pred_severity_classes = rnn_pred_severity.argmax(axis=1)\n",
    "\n",
    "y_val_cond_classes = y_val_cond_rnn.argmax(axis=1)\n",
    "y_val_sev_classes = y_val_sev_rnn.argmax(axis=1)\n",
    "\n",
    "print(\"\\n=== RNN Classification Reports ===\")\n",
    "print(\"\\n--- Condition Classification ---\")\n",
    "print(classification_report(y_val_cond_classes, rnn_pred_condition_classes))\n",
    "\n",
    "print(\"\\n--- Severity Classification ---\")\n",
    "print(classification_report(y_val_sev_classes, rnn_pred_severity_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for RNN model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Condition confusion matrix\n",
    "condition_cm_rnn = confusion_matrix(y_val_cond_classes, rnn_pred_condition_classes)\n",
    "sns.heatmap(condition_cm_rnn, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('RNN Condition Classification Confusion Matrix')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_xticklabels(['SCS', 'LNFN', 'RNFN', 'LSS', 'RSS'], rotation=45)\n",
    "axes[0].set_yticklabels(['SCS', 'LNFN', 'RNFN', 'LSS', 'RSS'], rotation=0)\n",
    "\n",
    "# Severity confusion matrix\n",
    "severity_cm_rnn = confusion_matrix(y_val_sev_classes, rnn_pred_severity_classes)\n",
    "sns.heatmap(severity_cm_rnn, annot=True, fmt='d', cmap='Greens', ax=axes[1])\n",
    "axes[1].set_title('RNN Severity Classification Confusion Matrix')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_xticklabels(['Normal/Mild', 'Moderate', 'Severe'], rotation=45)\n",
    "axes[1].set_yticklabels(['Normal/Mild', 'Moderate', 'Severe'], rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConfusion matrices generated for RNN model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the RNN model\n",
    "rnn_model_save_path = os.path.join('/kaggle/working/', 'rnn_lstm_model.h5')\n",
    "rnn_model.save(rnn_model_save_path)\n",
    "print(f\"RNN/LSTM model saved at {rnn_model_save_path}\")\n",
    "\n",
    "# Model comparison summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n1. EfficientNetB0 Model:\")\n",
    "print(\"   - Architecture: Pre-trained CNN with transfer learning\")\n",
    "print(\"   - Input: Single image analysis\")\n",
    "print(\"   - Strengths: Strong feature extraction, good baseline performance\")\n",
    "print(\"\\n2. RNN/LSTM Model:\")\n",
    "print(\"   - Architecture: Sequential model with Bidirectional LSTM\")\n",
    "print(\"   - Input: Sequence of images (temporal analysis)\")\n",
    "print(\"   - Strengths: Captures patterns across multiple slices\")\n",
    "print(\"   - Use case: Better for analyzing complete MRI series\")\n",
    "print(\"\\n3. Recommendation:\")\n",
    "print(\"   - Use EfficientNetB0 for: Single slice classification, faster inference\")\n",
    "print(\"   - Use RNN/LSTM for: Complete series analysis, temporal patterns\")\n",
    "print(\"   - Consider ensemble: Combine both models for best results\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model in Batches with Training and Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch-wise splitting and yielding is used to efficiently handle large datasets like DICOM images without exceeding memory limits. By processing data in smaller batches, we reduce memory usage, preventing system crashes or notebook restarts. This approach allows for incremental loading, splitting, and model training while keeping only a portion of the data in memory at any time. It ensures scalability and stability when working with high-dimensional image data, optimizing both resource utilization and performance during training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T18:21:18.246384Z",
     "iopub.status.busy": "2025-04-19T18:21:18.245235Z",
     "iopub.status.idle": "2025-04-19T19:10:49.815171Z",
     "shell.execute_reply": "2025-04-19T19:10:49.814390Z",
     "shell.execute_reply.started": "2025-04-19T18:21:18.246357Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "X_train_df, X_val_df, y_train_condition, y_val_condition, y_train_severity, y_val_severity = train_test_split(\n",
    "    X_smote_combined_df, \n",
    "    y_condition_smote_df_filtered.values, \n",
    "    y_severity_smote_df_filtered.values, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Confirm that the split has worked correctly\n",
    "print(f\"X_train_df size: {len(X_train_df)}, X_val_df size: {len(X_val_df)}\")\n",
    "print(f\"y_train_condition size: {len(y_train_condition)}, y_val_condition size: {len(y_val_condition)}\")\n",
    "print(f\"y_train_severity size: {len(y_train_severity)}, y_val_severity size: {len(y_val_severity)}\")\n",
    "\n",
    "# Callbacks: Early stopping and model checkpointing\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('efficientnetb0_model.keras', save_best_only=True)\n",
    "\n",
    "# Define the image folder\n",
    "image_folder = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_images/'\n",
    "\n",
    "# Create data generators for training and validation\n",
    "train_generator = DataGenerator(X_train_df, image_folder, y_train_condition, y_train_severity, batch_size=32)\n",
    "val_generator = DataGenerator(X_val_df, image_folder, y_val_condition, y_val_severity, batch_size=32)\n",
    "\n",
    "# Build the model (adjust num_classes_condition and num_classes_severity as needed)\n",
    "model = build_efficientnet_model(num_classes_condition=5, num_classes_severity=3)\n",
    "\n",
    "# Train the model with the data generators\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=10,  # Set your number of epochs\n",
    "    callbacks=[earlystop, checkpoint],\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Trained Model to the Output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T19:14:00.929424Z",
     "iopub.status.busy": "2025-04-19T19:14:00.929124Z",
     "iopub.status.idle": "2025-04-19T19:14:01.531279Z",
     "shell.execute_reply": "2025-04-19T19:14:01.530671Z",
     "shell.execute_reply.started": "2025-04-19T19:14:00.929400Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the trained model to the output directory\n",
    "output_directory = '/kaggle/working/'  # Output folder in Kaggle\n",
    "model_save_path = os.path.join(output_directory, 'efficientnetb0_model.h5')\n",
    "\n",
    "# Save the model\n",
    "model.save(model_save_path)\n",
    "\n",
    "print(f\"Model saved at {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T19:19:18.680587Z",
     "iopub.status.busy": "2025-04-19T19:19:18.680303Z",
     "iopub.status.idle": "2025-04-19T19:20:49.501400Z",
     "shell.execute_reply": "2025-04-19T19:20:49.500642Z",
     "shell.execute_reply.started": "2025-04-19T19:19:18.680568Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 32  # Set your desired batch size here\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "val_generator = DataGenerator(X_val_df, image_folder, y_val_condition, y_val_severity, batch_size=batch_size)\n",
    "\n",
    "# Perform evaluation, expecting 3 values: overall loss, condition accuracy, severity accuracy\n",
    "results = model.evaluate(val_generator)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve for Condition and Severity Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T19:21:01.049402Z",
     "iopub.status.busy": "2025-04-19T19:21:01.048802Z",
     "iopub.status.idle": "2025-04-19T19:22:44.420331Z",
     "shell.execute_reply": "2025-04-19T19:22:44.419544Z",
     "shell.execute_reply.started": "2025-04-19T19:21:01.049379Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Function to plot ROC curve\n",
    "def plot_roc_curve(y_true, y_pred, n_classes, title):\n",
    "    # Binarize the true labels for ROC\n",
    "    y_true_binarized = label_binarize(y_true, classes=range(n_classes))\n",
    "\n",
    "    # Compute ROC curve and AUC for each class\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_binarized[:, i], y_pred[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Plot ROC curves\n",
    "    plt.figure()\n",
    "    for i in range(n_classes):\n",
    "        plt.plot(fpr[i], tpr[i], label=f\"Class {i} (AUC = {roc_auc[i]:.2f})\")\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guessing\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "# Get the model predictions using the validation data generator\n",
    "y_pred_condition, y_pred_severity = model.predict(val_generator)\n",
    "\n",
    "# Plot ROC curves for condition classification\n",
    "plot_roc_curve(y_val_condition.argmax(axis=1), y_pred_condition, 5, \"ROC Curve for Condition Classification\")\n",
    "\n",
    "# Plot ROC curves for severity classification\n",
    "plot_roc_curve(y_val_severity.argmax(axis=1), y_pred_severity, 3, \"ROC Curve for Severity Classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix and Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T19:23:24.211706Z",
     "iopub.status.busy": "2025-04-19T19:23:24.211413Z",
     "iopub.status.idle": "2025-04-19T19:23:24.252933Z",
     "shell.execute_reply": "2025-04-19T19:23:24.252217Z",
     "shell.execute_reply.started": "2025-04-19T19:23:24.211683Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Convert predictions to class labels\n",
    "y_pred_condition_classes = y_pred_condition.argmax(axis=1)\n",
    "y_pred_severity_classes = y_pred_severity.argmax(axis=1)\n",
    "\n",
    "y_val_condition_classes = y_val_condition.argmax(axis=1)\n",
    "y_val_severity_classes = y_val_severity.argmax(axis=1)\n",
    "\n",
    "# Confusion Matrix for Condition Classification\n",
    "condition_cm = confusion_matrix(y_val_condition_classes, y_pred_condition_classes)\n",
    "print(\"Confusion Matrix for Condition Classification:\")\n",
    "print(condition_cm)\n",
    "\n",
    "# Confusion Matrix for Severity Classification\n",
    "severity_cm = confusion_matrix(y_val_severity_classes, y_pred_severity_classes)\n",
    "print(\"Confusion Matrix for Severity Classification:\")\n",
    "print(severity_cm)\n",
    "\n",
    "# Classification Report for Condition Classification\n",
    "condition_report = classification_report(y_val_condition_classes, y_pred_condition_classes)\n",
    "print(\"Classification Report for Condition Classification:\")\n",
    "print(condition_report)\n",
    "\n",
    "# Classification Report for Severity Classification\n",
    "severity_report = classification_report(y_val_severity_classes, y_pred_severity_classes)\n",
    "print(\"Classification Report for Severity Classification:\")\n",
    "print(severity_report)\n",
    "\n",
    "# Overall Accuracy for both tasks\n",
    "condition_accuracy = accuracy_score(y_val_condition_classes, y_pred_condition_classes)\n",
    "severity_accuracy = accuracy_score(y_val_severity_classes, y_pred_severity_classes)\n",
    "\n",
    "print(f\"Condition Classification Accuracy: {condition_accuracy:.2f}\")\n",
    "print(f\"Severity Classification Accuracy: {severity_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T19:23:36.656561Z",
     "iopub.status.busy": "2025-04-19T19:23:36.655836Z",
     "iopub.status.idle": "2025-04-19T19:23:36.665382Z",
     "shell.execute_reply": "2025-04-19T19:23:36.664603Z",
     "shell.execute_reply.started": "2025-04-19T19:23:36.656537Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Modified Data Generator class for test data using study_id and series_id from test_desc\n",
    "class TestDataGenerator(Sequence):\n",
    "    def __init__(self, df, image_folder, batch_size=32, img_size=(224, 224)):\n",
    "        self.df = df  # Use the test_desc DataFrame\n",
    "        self.image_folder = image_folder\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.indices = np.arange(len(df))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Generate batch indices\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        \n",
    "        # Get the batch data\n",
    "        batch_df = self.df.iloc[batch_indices]\n",
    "        \n",
    "        # Load the images for the batch\n",
    "        images = []\n",
    "        for _, row in batch_df.iterrows():\n",
    "            study_id = int(row['study_id'])  # Ensure integer format\n",
    "            series_id = int(row['series_id'])  # Ensure integer format\n",
    "            \n",
    "            # Construct image path using integer values\n",
    "            series_path = os.path.join(self.image_folder, str(study_id), str(series_id))\n",
    "            dicom_files = [f for f in os.listdir(series_path) if f.endswith('.dcm')]\n",
    "            \n",
    "            # Load the first image in the series (adjust if needed)\n",
    "            if dicom_files:\n",
    "                img_path = os.path.join(series_path, dicom_files[0])\n",
    "                img = self.load_dicom_image(img_path)\n",
    "                images.append(img)\n",
    "            else:\n",
    "                print(f\"Warning: No DICOM files found for series {series_id} in study {study_id}\")\n",
    "                images.append(np.zeros((*self.img_size, 3)))  # Blank image if no DICOM\n",
    "            \n",
    "        return np.array(images)\n",
    "    \n",
    "    def load_dicom_image(self, image_path):\n",
    "        try:\n",
    "            dicom = pydicom.dcmread(image_path)\n",
    "            if hasattr(dicom, 'pixel_array'):\n",
    "                img = dicom.pixel_array\n",
    "                img = cv2.resize(img, self.img_size)  # Resize to 224x224\n",
    "                img = np.stack((img,) * 3, axis=-1)  # Convert to RGB (3 channels)\n",
    "                img = img / 255.0  # Normalize\n",
    "                return img\n",
    "            else:\n",
    "                print(f\"Warning: No pixel data in DICOM file: {image_path}\")\n",
    "                return np.zeros((*self.img_size, 3))  # Return a blank image if no pixel data\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading DICOM file: {image_path}. Error: {e}\")\n",
    "            return np.zeros((*self.img_size, 3))  # Return a blank image if any error occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T19:23:41.231121Z",
     "iopub.status.busy": "2025-04-19T19:23:41.230605Z",
     "iopub.status.idle": "2025-04-19T19:23:48.347353Z",
     "shell.execute_reply": "2025-04-19T19:23:48.346748Z",
     "shell.execute_reply.started": "2025-04-19T19:23:41.231098Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the test image folder\n",
    "test_image_folder = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/test_images/'\n",
    "\n",
    "# Create the test data generator using the test_desc DataFrame\n",
    "test_generator = TestDataGenerator(test_desc, test_image_folder, batch_size=32)\n",
    "\n",
    "# Get model predictions for the test set\n",
    "y_pred_condition, y_pred_severity = model.predict(test_generator)\n",
    "\n",
    "# Since there are no true labels, just output predictions\n",
    "print(f\"Predicted Conditions: {y_pred_condition}\")\n",
    "print(f\"Predicted Severity: {y_pred_severity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakdown of Predicted Conditions and Severities:\n",
    "\n",
    "The model predicts both a condition class and a severity level for each sample. Here, we interpret the predicted probabilities:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicted Conditions**:\n",
    "\n",
    "Each row corresponds to the model's prediction for a test sample, with probabilities assigned to each of the five condition classes (`condition_0` to `condition_4`), which represent specific lumbar spine conditions. The class with the highest probability is considered the model's prediction.\n",
    "\n",
    "Sample 1:\n",
    "`[45.9%, 0.16%, 8.15%, 0.11%, 45.7%]` \n",
    "The model is almost equally confident between **Spinal Canal Stenosis** (45\\.9%) and **Right Subarticular Stenosis** (45\\.7%).\n",
    "\n",
    "Sample 2:\n",
    "`[0.009%, 67.08%, 0.008%, 32.9%, 0.005%]`\n",
    "The model strongly predicts **Left Neural Foraminal Narrowing** (67.08%), but also assigns a considerable probability to **Left Subarticular Stenosis** (32.9%). Despite the presence of the second-highest score, Left Neural Foraminal Narrowing is clearly favored.\n",
    "\n",
    "Sample 3:\n",
    "`[0.0000008%, 0.00000029%, 0.00000031%, 0.000000086%, 100%]`\n",
    "The model is **100%** confident in **Right Subarticular Stenosis**. This is an extremely confident prediction, with no uncertainty about the correct condition class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicted Severities**:\n",
    "\n",
    "Each row also corresponds to the model's prediction of severity for the given condition, with probabilities assigned to three severity levels (severity_0 to severity_2). The class with the highest probability is considered the predicted severity level.\n",
    "\n",
    "Sample 1: \n",
    "`[19.68%, 43.98%, 36.35%]`\n",
    "The model is moderately confident in **Normal/Mild** (44%), but it also assigns a significant probability to **Moderate** (36.35%). This indicates some uncertainty in the severity classification.\n",
    "\n",
    "\n",
    "Sample 2:\n",
    "\n",
    "`[4.6%, 94.2%, 1.2%]`\n",
    "The model is highly confident that this sample corresponds to **Normal/Mild** (94.2%), with almost no chance of it being any other severity.\n",
    "\n",
    "Sample 3:\n",
    "\n",
    "`[0.92%, 3.02%, 96.05%]`\n",
    "The model is highly confident in predicting **Moderate** (~96%), with minimal doubt about the severity level."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8561470,
     "sourceId": 71549,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
